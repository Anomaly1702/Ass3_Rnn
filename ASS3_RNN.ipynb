{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab2cb482",
      "metadata": {
        "id": "ab2cb482"
      },
      "source": [
        "## Define Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1aecab",
      "metadata": {
        "id": "1b1aecab"
      },
      "outputs": [],
      "source": [
        "import math # Mathematical functions \n",
        "import numpy as np # Fundamental package for scientific computing with Python\n",
        "import pandas as pd # Additional functions for analysing and manipulating data\n",
        "from datetime import date, timedelta, datetime # Date Functions\n",
        "from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\n",
        "import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data\n",
        "import matplotlib.dates as mdates # Formatting dates\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\n",
        "from keras import Sequential # Deep learning library, used for neural networks\n",
        "from keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\n",
        "from keras.callbacks import EarlyStopping # EarlyStopping during model training\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, SimpleRNN , Dropout, LSTM, GRU\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data \n",
        "import seaborn as sns # Visualization\n",
        "sns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})\n",
        "%matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/Deep_Learning/MSFT.csv'\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fb7117",
      "metadata": {
        "id": "47fb7117"
      },
      "outputs": [],
      "source": [
        "# import yfinance\n",
        "# msft = yf.download(MSFT, start=start_date, end=end_date)\n",
        "# hist = msft.history(period='max')\n",
        "# hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd72db1",
      "metadata": {
        "id": "5dd72db1"
      },
      "outputs": [],
      "source": [
        "MSFT = pd.read_csv(data_dir)\n",
        "# MSFT = MSFT.iloc[7000:,:]\n",
        "MSFT\n",
        "MSFT = MSFT.set_index('Date')\n",
        "display(MSFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1d3dd2",
      "metadata": {
        "id": "ab1d3dd2"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "fig.add_trace(go.Scatter(x=MSFT.index,y=MSFT['Adj Close'],name='Closing Price'),secondary_y=False)\n",
        "# fig.add_trace(go.Bar(x=MSFT['Date'],y=MSFT['Volume'],name='Volume'),secondary_y=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86aab1c",
      "metadata": {
        "id": "b86aab1c"
      },
      "outputs": [],
      "source": [
        "# Plot line charts\n",
        "df_plot = MSFT.copy()\n",
        "df_plot.plot()\n",
        "# ncols = 2\n",
        "# nrows = int(round(df_plot.shape[1] / ncols, 0))\n",
        "\n",
        "# fig, ax = plt.subplots(nrows=3, ncols=2, sharex=True, figsize=(14, 7))\n",
        "# for i, ax in enumerate(fig.axes):\n",
        "#         sns.lineplot(data = df_plot.iloc[:, i+1], ax=ax)\n",
        "#         # ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
        "#         # ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "# fig.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bd27ad",
      "metadata": {
        "id": "16bd27ad"
      },
      "outputs": [],
      "source": [
        "class Basenet():\n",
        "    def __init__(self,index_Close):\n",
        "        self.train_set = None\n",
        "        self.val_set = None\n",
        "        self.test_set = None\n",
        "        self.index_Close = index_Close\n",
        "\n",
        "    def partition_dataset(self,sequence_length, data):\n",
        "        x, y = [], []\n",
        "        data_len = data.shape[0]\n",
        "        for i in range(sequence_length, data_len):\n",
        "            x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
        "            y.append(data[i, self.index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
        "        \n",
        "        # Convert the x and y to numpy arrays\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "        return x, y\n",
        "\n",
        "    def split_data(self,stock,splitsize):\n",
        "        # data_raw = stock.to_numpy() # convert to numpy array\n",
        "        data = stock.copy()\n",
        "        #calc test size\n",
        "        data = np.array(data)\n",
        "        test_size = int(np.round(len(data)*splitsize))\n",
        "        bg_train_size = len(data) - test_size\n",
        "        val_size = int(np.round(bg_train_size*(splitsize/2)))\n",
        "        train_size = bg_train_size - val_size\n",
        "        # test_set_size = int(np.round(0.2*data.shape[0]))\n",
        "        # train_set_size = data.shape[0] - (test_set_size)\n",
        "        print(f\"Train :{train_size}, Val :{val_size}, test :{test_size}\")\n",
        "        print(\"bg_train :\",bg_train_size)\n",
        "        self.train_set = data[:train_size,:]\n",
        "        self.val_set = data[train_size:bg_train_size,:]\n",
        "        self.test_set = data[bg_train_size:,:]\n",
        "#         x_train = data[:train_size,:-1]\n",
        "#         y_train = data[:train_size,-1]\n",
        "        print(\"Training Size :\",len(self.train_set))\n",
        "        print(\"Validation Size :\",len(self.val_set))\n",
        "#         x_test = data[train_size:,:-1]\n",
        "#         y_test = data[train_size:,-1]\n",
        "        print(\"Testing Size :\",len(self.test_set))\n",
        "        # return [x_train, y_train, x_test, y_test]\n",
        "        return True\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing\n",
        "\n",
        ">  we drop Columns Date since this does not give valuable information regarding stock price instead is just a measure of time passsing\n",
        "\n",
        "> we drop Adjusted close since it is calculated after market closing post divident claculation and is not a continoues flow .\n",
        "\n"
      ],
      "metadata": {
        "id": "zqueMbNiE_xh"
      },
      "id": "zqueMbNiE_xh"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Indexing Batches\n",
        "# train_df = df.sort_values(by=['Date']).copy()\n",
        "filtered_df = MSFT.copy()\n",
        "# Removing Adjusted Close Column\n",
        "filtered_df = filtered_df.drop([ 'Adj Close'], axis=1)\n",
        "filtered_df['Prediction'] = filtered_df['Close']\n",
        "# Print the tail of the dataframe\n",
        "display(filtered_df.tail())\n",
        "\n",
        "#Using Min-Max scaling to make data more uniform\n",
        "\n",
        "# Get the number of rows in the data\n",
        "# nrows = data_filtered.shape[0]\n",
        "\n",
        "# # Convert the data to numpy values\n",
        "# np_data_unscaled = np.array(data_filtered)\n",
        "# np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
        "# print(np_data.shape)\n",
        "\n",
        "# Transform the data by scaling each feature to a range between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(filtered_df)\n",
        "display(data_scaled[7500:,:])\n",
        "# Creating a separate scaler that works on a single column for scaling predictions\n",
        "scaler_pred = MinMaxScaler()\n",
        "df_Close = pd.DataFrame(filtered_df['Close'])\n",
        "np_Close_scaled = scaler_pred.fit_transform(df_Close)"
      ],
      "metadata": {
        "id": "GGKwLNe5E-ib"
      },
      "id": "GGKwLNe5E-ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2d0e1a",
      "metadata": {
        "id": "9c2d0e1a"
      },
      "outputs": [],
      "source": [
        "l = Basenet()\n",
        "l.split_data(data_scaled, 20,.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla RNN"
      ],
      "metadata": {
        "id": "06TAlgu-Du--"
      },
      "id": "06TAlgu-Du--"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTM_cls(Basenet):\n",
        "\n",
        "  def __init__(self,index_Close):\n",
        "    super().__init__(index_Close)\n",
        "    self.optimizer = None\n",
        "    self.x_train = None\n",
        "\n",
        "  def train(self,model,sequence_length):\n",
        "    x_train,y_train = self.partition_dataset(sequence_length,self.train_set)\n",
        "    x_test,y_test = self.partition_dataset(sequence_length,self.test_set)\n",
        "    print(f\"trainx : {x_train.shape}\")\n",
        "    print(f\"testx : {x_test.shape}\")\n",
        "    print(f\"y_train : {y_train.shape}\")\n",
        "    print(f\"y_test : {y_test.shape}\")\n",
        "\n",
        "    # Training the model\n",
        "    epochs = 50\n",
        "    batch_size = 16\n",
        "    early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "    history = model.fit(x_train, y_train, \n",
        "                        batch_size=batch_size, \n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test)\n",
        "                      )\n",
        "                        \n",
        "                        # callbacks=[early_stop])\n",
        "    return history\n",
        "\n",
        "  def LSTM_model(self,sequence_length,optimizer = 'adam',lr=0.001):\n",
        "    x_train,y_train = self.partition_dataset(sequence_length,self.train_set)\n",
        "    self.x_train = x_train\n",
        "    if optimizer == 'adam':\n",
        "      self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif optimizer == 'sgd': \n",
        "      self.optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.0)\n",
        "\n",
        "    model = Sequential()\n",
        "    # Model with n_neurons = inputshape Timestamps, each with self.train_set.shape[2] variables\n",
        "    n_neurons = self.x_train.shape[1] * self.x_train.shape[2]\n",
        "    # print(n_neurons, self.train_set.shape[1], self.train_set.shape[2])\n",
        "    model.add(\n",
        "        LSTM(\n",
        "            n_neurons, return_sequences=True, \n",
        "             input_shape=(self.x_train.shape[1], self.x_train.shape[2]))\n",
        "        ) \n",
        "    model.add(\n",
        "        LSTM(n_neurons, return_sequences=False)\n",
        "        )\n",
        "    model.add(Dense(5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=self.optimizer, loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "9cGNOHH3BjhH"
      },
      "id": "9cGNOHH3BjhH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.train_set[:, -1][:,3].shape"
      ],
      "metadata": {
        "id": "DygHDXOJuSxg"
      },
      "id": "DygHDXOJuSxg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM_cls(3)\n",
        "lstm.split_data(data_scaled,.2)\n",
        "# display(lstm.train_set.shape)\n",
        "# lstm.train_set.shape[1] * lstm.train_set.shape[2]\n",
        "lstm_model = lstm.LSTM_model(20)\n",
        "history = lstm.train(lstm_model,20)\n",
        "# Plot training & validation loss values\n",
        "fig, ax = plt.subplots(figsize=(16, 5), sharex=True)\n",
        "sns.lineplot(data=history.history[\"loss\"])\n",
        "plt.title(\"Model loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "ax.xaxis.set_major_locator(plt.MaxNLocator(50))\n",
        "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GJQWTZyRd837"
      },
      "id": "GJQWTZyRd837",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QDCVAcvjQ8F"
      },
      "id": "9QDCVAcvjQ8F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}